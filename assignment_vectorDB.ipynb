{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# VectorDB, Hugging Face & Ollama \u2013 Assignment Solutions\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 1 Answer\nDetailed explanation of VectorDB vs Traditional DBs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 2 Answer\nTypes of VectorDBs and their use cases."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 3 Answer\nWhy ChromaDB is important and its features."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 4 Answer\nBenefits of Hugging Face Hub."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Question 5 Answer\nProcess & advantages of using pre-trained models."]}, {"cell_type": "code", "metadata": {}, "source": ["# Question 6: ChromaDB setup and sample vector\n!pip install chromadb\n\nfrom chromadb import Client\nfrom chromadb.config import Settings\n\nclient = Client(Settings(chroma_db_impl=\"duckdb+parquet\", persist_directory=\"my_chroma_db\"))\ncollection = client.create_collection(\"sample_collection\")\n\ncollection.add(\n    ids=[\"1\",\"2\",\"3\"],\n    embeddings=[[0.1,0.2,0.3],[0.9,0.8,0.7],[0.4,0.4,0.5]],\n    documents=[\"apple fruit\", \"car engine\", \"banana yellow\"]\n)\n\nresults = collection.query(query_embeddings=[[0.12,0.18,0.33]], n_results=2)\nresults"]}, {"cell_type": "code", "metadata": {}, "source": ["# Question 7: Fine-tuning Hugging Face model\n!pip install transformers datasets accelerate\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom datasets import load_dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized = dataset.map(tokenize, batched=True)\n\ntraining_args = TrainingArguments(output_dir=\"./fine_tuned_gpt2\", num_train_epochs=1)\ntrainer = Trainer(model=model, args=training_args, train_dataset=tokenized[\"train\"])\n# trainer.train()  # commented to avoid long runtime"]}, {"cell_type": "code", "metadata": {}, "source": ["# Question 8: Custom LLM using Ollama\n# Commands to run in terminal:\n# curl -fsSL https://ollama.com/install.sh | sh\n# Create Modelfile and run:\n# ollama create myllama -f Modelfile\n# ollama run myllama \"Explain AI\""]}, {"cell_type": "code", "metadata": {}, "source": ["# Question 9: Basic RAG using Ollama + ChromaDB\nimport chromadb, subprocess\nfrom chromadb.config import Settings\n\nclient = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\", persist_directory=\"rag_db\"))\ncollection = client.create_collection(\"rag_docs\")\ncollection.add(ids=[\"1\"], documents=[\"Machine learning allows computers to learn patterns.\"])\n\nquery = \"What is machine learning?\"\nresults = collection.query(query_texts=[query], n_results=1)\ncontext = results[\"documents\"][0][0]\n\nprompt = f\"Use the context to answer:\\n{context}\\n\\nQuestion: {query}\"\n# result = subprocess.run([\"ollama\", \"run\", \"llama3\"], input=prompt, text=True, capture_output=True)\n# print(result.stdout)"]}, {"cell_type": "code", "metadata": {}, "source": ["# Question 10: Health-tech chatbot architecture with HF + VectorDB + Ollama\nfrom transformers import AutoTokenizer, AutoModel\nimport chromadb\n\ntokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-bert\")\nmodel = AutoModel.from_pretrained(\"d4data/biomedical-bert\")\n\ndef get_embedding(text):\n    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True)\n    output = model(**tokens)\n    return output.last_hidden_state.mean(dim=1).detach().numpy().tolist()[0]\n\nclient = chromadb.Client()\ncollection = client.create_collection(\"medical_docs\")\ncollection.add(ids=[\"1\"], documents=[\"Jaundice is caused by bilirubin buildup.\"])\n\nquery = \"What causes jaundice?\"\nresults = collection.query(query_texts=[query], n_results=1)\nresults"]}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 5}